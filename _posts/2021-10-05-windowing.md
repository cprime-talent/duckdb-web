---

layout: post
title:  "Windowing in DuckDB"
author: Richard Wesley
excerpt_separator: <!--more-->

---

_TLDR: DuckDB, a free and Open-Source analytical data management system, has a state-of-the-art window computation engine._

Window functions (those using the `OVER` clause) are important tools for analysing ordered data,
but they can be slow if not implemented carefully.
In this post, we will take a look at how DuckDB implements windowing.

<!--more-->

## Beyond Sets

The original relational model as developed by Codd in the 1970s treated relations as *unordered sets* of tuples.
While this was nice for theoretical computer science work,
it ignored the way humans think using physical analogies (the "embodied brain" model from neuroscience).
In particular, humans naturally order data to help them understand it and engage with it.
To help with this, SQL uses the `SELECT` clause for horizontal layout and the `ORDER BY` clause for vertical layout.

Still, the orderings that humans put on data are often more than neurological crutches.
For example, time places a natural ordering on measurements,
and wide swings in those measurements can themselves be important data,
or they may indicate that the data needs to be cleaned by smoothing.
Trends may be present or relative changes may be more important for analysis than raw values.
To help answer such questions, SQL introduced *analytic* (or *window*) functions in 2003.

### Window Functions

Windowing breaks a relation up into independent *partitions*, *orders* those partitions,
and then defines [various functions](/docs/sql/window_functions) that can be computed for each row.
These functions include all aggregate functions (such as `SUM` and `AVG`)
as well as some window-specific functions (such as `RANK()` and `NTH_VALUE(<expression>, <N>)` ).

Some window functions depend only on the partition boundary and the ordering,
but a few (including all the aggregates) also use a *frame*.
Frames are specified as a number of rows on either side (*preceding* or *following* the *current row*).
The distance can either be specified as a number of *rows* or a *range* of values
using the partition's ordering value and a distance.

<img src="/images/blog/windowing/framing.png" alt="The Window Computation Environment" title="Figure 1: The Window Computation Environment" style="max-width:70%;align:middle"/>

Let's look at an example of a window function query:

```sql
SELECT "Plant", "Date",
    AVG("KWh") OVER (
        PARTITION BY "Plant"
        ORDER BY "Date" ASC
        RANGE BETWEEN INTERVAL 3 DAYS PRECEDING
                  AND INTERVAL 3 DAYS FOLLOWING)
        AS "KWh 7-day Moving Average"
FROM "Generation History"
ORDER BY 1, 2
```

This query computes the seven day moving average of the energy generated by each power plant on each day.
It partitions the data by `Plant` (to keep the different power plants' data separate),
orders each plant's partition by `Date` (to put the energy measurements next to each other),
and uses a `RANGE` frame of three days on either side of each day for the `AVG`
(to handle any missing days).

The `OVER` clause is the way that SQL specifies that a function is to be computed in a window.
You can request multiple different `OVER` clauses in the same `SELECT`, and each will be computed separately.
Often, however, you want to use the same window for multiple functions,
and you can do this by using a `WINDOW` clause to define a *named* window:

```sql
SELECT "Plant", "Date", AVG("KWh") OVER seven AS "KWh 7-day Moving Average"
FROM "Generation History"
WINDOW seven AS (
    PARTITION BY "Plant"
    ORDER BY "Date" ASC
    RANGE BETWEEN INTERVAL 3 DAYS PRECEDING
              AND INTERVAL 3 DAYS FOLLOWING)
ORDER BY 1, 2
```

## Under the Feathers

That is a long list of complicated functionality!
Making it all work relatively quickly has many pieces,
so lets have a look at how they all get implemented in DuckDB.

### Pipeline Breaking

The first thing to notice is that windowing is a "pipeline breaker".
That is, the `Window` operator has to read all of its inputs before it can start computing a function.
This means that if there is some other way to compute something,
it may well be faster to use a different technique.

One common analytic task is to find the last value in some group.
It is tempting to use the `RANK()` window function with a reverse sort for this task:

```sql
SELECT entity, value
FROM (
    SELECT entity, value,
        RANK() OVER (PARTITION BY entity ORDER BY date DESC) AS r
    FROM table) t
WHERE r = 1
```

but this requires materialising the entire table, partitioning it, sorting the partitions,
and then pulling out a single row from those partitions.
A much faster way to do this is to use a self join to filter the table:

```sql
SELECT entity, value
FROM table,
     (SELECT entity, MAX(date) AS date FROM table GROUP BY 1) dates
WHERE table.entity = dates.entity
  AND table.date = dates.date
```

This query requires two scans of the table, but the only materialised data is the filtering table
(which is probably much smaller than the original table), and there is no sorting at all.
This showed up [in a user's blog](https://bwlewis.github.io/duckdb_and_r/last/last.html)
and we found that the second query was over 20 times faster:

<img src="/images/blog/windowing/last-in-group.jpg" alt="Window takes 13 seconds, Join takes half a second" title="Last in Group Performance Comparison" style="max-width:70%"/>

Of course most analytic tasks that use windowing *do* require using the `Window` operator,
and DuckDB uses a collection of modern techniques to make the performance as fast as possible.

### Partitioning and Sorting

At one time, windowing was implemented by sorting on both the partition and the ordering fields.
This is resource intensive, both because the entire relation must be sorted,
and because sorting is `O(N log N)` in the size of the relation.
Fortunately, there are faster ways to implement this step.

To reduce resource consumption, DuckDB uses the hash partitioning scheme from Leis et al.'s
[*Efficient Processing of Window Functions in Analytical SQL Queries*](http://www.vldb.org/pvldb/vol8/p1058-leis.pdf)
and breaks the partitions up into 1024 chunks using hashing.
The chunks still need to be sorted on all the fields because there may be hash collisions,
but each partition can now be 1024 times smaller, which reduces the runtime significantly.
Moreover, the partitions can easily be extracted and processed in parallel.

Sorting in DuckDB recently got a [big performance boost](/2021/08/27/external-sorting)
along with the ability to work on partitions that were larger than memory.
This functionality has been also added to the `Window` operator,
resulting in a 33% improvement in the last-in-group example:

<img src="/images/blog/windowing/last-in-group-sort.jpg" alt="Window takes X seconds, Join takes half a second" title="Last in Group Sorting Performance Comparison" style="max-width:70%"/>

Even though you can request multiple window functions,
DuckDB will collect functions that use the same partitioning and ordering
and share the layout of the data between those functions.

### Aggregation

Most of the [general-purpose window functions](/docs/sql/window_functions) are straightforward to compute,
but windowed aggregate functions can be expensive because they need to look at multiple values for each row.
They often need to look at the same value multiple times, or repeatedly look at a large number of values,
so over the years several approaches have been taken to improve performance.

#### Basic Windowed Aggregation

Before explaining how DuckDB implements windowed aggregation,
we need to take a short detour through how regular aggregates are implemented.
Aggregate "functions" are implemented using a three required operations and one optional operation:
* *Initialize* - Creates a state that will be updated.
For `SUM`, this is the running total, starting at `NULL` (because a `SUM` of zero items is `NULL`, not zero.)
* *Update* - Updates the state with a new value. For `SUM`, this adds the value to the state.
* *Finalize* - Produces the final aggregate value from the state.
For `SUM`, this just copies the running total.
* *Combine* - Combines two states into a single state.
This is optional, but when present it allows the aggregate to be computed in parallel.
For `SUM`, this produces a new state with the sum of the two input values.

The simplest way to compute a windowed aggregate value is to *initialize* a state,
*update* the state with all the values in the window frame,
and then use *finalize* to produce the value of the windowed aggregate.
This na√Øve algorithm will always work, but it is quite inefficient.
For example, a running total will re-add all the values from the start of the partition
for each aggregate, which has a run time of `O(N^2)`.

To improve on this, some databases add additional
["moving state" operations](https://www.postgresql.org/docs/14/sql-createaggregate.html)
that can add or remove individual values incrementally.
This is an improvement, but it can only be used for certain aggregates (for example, it doesn't work for `MIN`).
Moreover, if the frame boundaries move around a lot, it can still degenerate to an `O(N^2)` run time.

#### Segment Tree Aggregation

Instead, DuckDB uses the *segment tree* approach from Leis et al. above.
This works by building a tree on top of the entire partition with the aggregated values at the bottom.
Values are combined into states at nodes above them in the tree until there is a single root:

<img src="/images/blog/windowing/segment-tree.png" alt="Segment Tree for SUM aggregation" title="Figure 5: Segment Tree for sum aggregation. Only the red nodes (7, 13, 20) have to be aggregated to compute the sum of 7, 3, 10, 6, 2, 8, 4" style="max-width:70%"/>

To compute a value, the algorithm generates states for the ragged ends of the frame,
*combines* states in the tree above the values in the frame,
and *finalizes* the result from the last remaining state.
This can be used for all combinable aggregates.

#### General Windowed Aggregation

The biggest drawback of segment trees is the need to manage a potentially large number of intermediate states.
For the simple states used for standard aggregates like `SUM`,
this is not a problem because the states are small,
the tree keeps the number of states logarithmically low,
and the state used to compute each value is also cheap.

For some aggregates, however, the state is not small.
Typically these are so-called *holistic* aggregates,
where the value depends on all the values of the frame.
Examples of such aggregates are `mode` and `quantile`,
where each state may have to contain a copy of *all* the values seen so far.
For large frames, this can be quite expensive.

To solve this problem, we use the approach from Wesley and Xu's
[*Incremental Computation of Common Windowed Holistic Aggregates*](http://www.vldb.org/pvldb/vol9/p1221-wesley.pdf),
which generalises segment trees to aggregate-specific data structures.
The aggregate can define a fifth optional *window* operation,
which will be passed the bottom of the tree and the bounds of the current and previous frame.
The aggregate can then create an appropriate data structure for its implementation.
For example, the `mode` function maintains a hash table of counts that it can update efficiently,
and the `quantile` function maintains a partially sorted list of frame indexes.

### Ordered Set Aggregates

Window functions are often closely associated with some special
["ordered set aggregates"](https://www.postgresql.org/docs/current/functions-aggregate.html#FUNCTIONS-ORDEREDSET-TABLE)
defined by the SQL standard.
Some databases implement these functions using the `Window` operator,
but this is rather inefficient as sorting the data (an `O(N log N)` operation) is not actually required.

Instead, DuckDB has efficient implementations of `quantile_cont`, `quantile_disc`,
and `mode` that use hash tables or selection, which have run time of `O(N)`.
Moreover, the `quantile` functions can take an array of quantile values,
which further increases performance by sharing the partially ordered results
among the different quantile values.

Because these are combinable aggregates, they can themselves be used in a windowing context,
so the moving average example above can be modified to produce a moving inter-quartile range:

```sql
SELECT "Plant", "Date",
    QUANTILE_CONT("KWh", [0.25, 0.5, 0.75]) OVER seven AS "KWh 7-day Moving IQR"
FROM "Generation History"
WINDOW seven AS (
    PARTITION BY "Plant"
    ORDER BY "Date" ASC
    RANGE BETWEEN INTERVAL 3 DAYS PRECEDING
              AND INTERVAL 3 DAYS FOLLOWING)
ORDER BY 1, 2
```

Moving quantiles are
[more robust to anomalies](https://blogs.sas.com/content/iml/2021/05/26/running-median-smoother.html),
which makes them a valuable tool for data series analysis.
And they are built into DuckDB's window operator.
